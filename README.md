# Synthetic-Image-Detection-Project
Our project revolves around creating a clever system that can classify images into two categories: those 
captured by cameras and those generated by computers. We're employing advanced computer techniques to 
closely examine images and understand what distinguishes real ones from synthetic ones. Our aim is to 
develop a tool that assists people in verifying the authenticity of images, which could be handy for checking 
online posts and unraveling digital puzzles.
To achieve this goal, we have used Convolutional Neural Networks (CNNs), which extract meaningful 
features from the images.The dataset consist of image files and each image file represents a specific real life objects. 
The dataset is split in half: one part contains 700 real images taken by cameras, while the other part contains 
700 synthetic images which are generated by Generative models like Diffusion models, Generative 
Adverserial Network (GANs), VAE etc. Initially, we focused on using only synthetic images generated by 
Diffusion models. These models create images using complex algorithms, mimicking real-world scenes. 
And further included the images generated by, Generative Adverserial Network (GANs). From total dataset 
we used 1161 images for training and 239 images for testing. For real images Common Objects in COntext-stuff (COCO-stuff) And for synthetic images 
we are using the subset of a dataset called "ArtiFact: Real and Fake Image Dataset”[link], which includes 
a variety of image files. These images are all sized at 200×200 pixels and cover a range of subjects, 
including people, animals, objects, food, and nature scenes. Eventually, we included digital arts also for our 
analysis.

The trained model is based on architecture of Convolutional Neural Network (CNN).This project introduces a Convolutional Neural Network architecture model consisting of 16 layers, 
which include convolutional, pooling, and fully connected layers. The model starts with a convolutional 
layer, followed by another convolutional layer, a dropout layer, and a max pooling layer. This pattern 
repeats twice before connecting to a flatten layer, which feeds into a fully connected layer and finally, the 
output layer.

In our project, we explored the integration of Fast Fourier Transforms (FFTs) with Convolutional 
Neural Networks (CNNs) to enhance accuracy in image classification tasks. By passing FFTs through CNNs 
instead of raw images, we achieved a significant improvement in accuracy, reaching an impressive 82.84%. 
To accomplish this integration, we employed the "Adam" optimizer and measured loss using binary crossentropy. Adam optimizer combines the strengths of AdaGrad and momentum optimizers, making it wellsuited for our task. Additionally, binary cross-entropy is utilized for its effectiveness in binary classification tasks.In conclusion, our study demonstrates the effectiveness of integrating FFTs with CNNs for image classification, showcasing the potential for improved accuracy and performance in real-world applications. Although FFTs are noise patterns of the images but they still affect the adequacy of the model according to visual difference of real and synthetic images.

